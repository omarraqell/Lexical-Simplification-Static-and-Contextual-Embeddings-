{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae481556485f47f2b6545a9125f3e7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66c931ca6a004e97ae130f9dcd69da5f",
              "IPY_MODEL_9ee420d75eb64cdeaaa914bf9a45a215",
              "IPY_MODEL_a84db6fdf3334a7cb04f8c0e750ce771"
            ],
            "layout": "IPY_MODEL_d94cede4d3814705b64b0dc37a1d3495"
          }
        },
        "66c931ca6a004e97ae130f9dcd69da5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d3b7708cd5499591c2329c73797b97",
            "placeholder": "​",
            "style": "IPY_MODEL_fb23deebf6e34c76af66e90b628ac216",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9ee420d75eb64cdeaaa914bf9a45a215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_545a39e5447c460a87526c9169e383ea",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85999f0c5e27429fafc1c6b24e70882b",
            "value": 2
          }
        },
        "a84db6fdf3334a7cb04f8c0e750ce771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fd017aa72c2497cbfbfd6e25003e317",
            "placeholder": "​",
            "style": "IPY_MODEL_09c984b4185e46f4a53621dd8c221521",
            "value": " 2/2 [00:05&lt;00:00,  2.73s/it]"
          }
        },
        "d94cede4d3814705b64b0dc37a1d3495": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84d3b7708cd5499591c2329c73797b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb23deebf6e34c76af66e90b628ac216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "545a39e5447c460a87526c9169e383ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85999f0c5e27429fafc1c6b24e70882b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fd017aa72c2497cbfbfd6e25003e317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09c984b4185e46f4a53621dd8c221521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "511f190358a145e6992b376e8d60831c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8c24199b8494166a8eeac4354e6d26f",
              "IPY_MODEL_4b418f79b9d34bbc8e91711d10860aa0",
              "IPY_MODEL_1d060af784604ce79972b9d9c83dcb47"
            ],
            "layout": "IPY_MODEL_dde55c72988843ea8e09546dd1778348"
          }
        },
        "e8c24199b8494166a8eeac4354e6d26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f0bf5b4e22e4b46b1990863e5da9ec5",
            "placeholder": "​",
            "style": "IPY_MODEL_b334563559a844c7aee0f2f95749243c",
            "value": "config.json: 100%"
          }
        },
        "4b418f79b9d34bbc8e91711d10860aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5d02c1e760b4a589106dd47dd72eada",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93cade46024c48758c5a3b5c8a7c0ad1",
            "value": 570
          }
        },
        "1d060af784604ce79972b9d9c83dcb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a82694b37ec245048ef9dd5a3541cffc",
            "placeholder": "​",
            "style": "IPY_MODEL_396687ee1d6248eda561684a94f64f01",
            "value": " 570/570 [00:00&lt;00:00, 60.6kB/s]"
          }
        },
        "dde55c72988843ea8e09546dd1778348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f0bf5b4e22e4b46b1990863e5da9ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b334563559a844c7aee0f2f95749243c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5d02c1e760b4a589106dd47dd72eada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93cade46024c48758c5a3b5c8a7c0ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a82694b37ec245048ef9dd5a3541cffc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "396687ee1d6248eda561684a94f64f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "454af9db73804a1a80289b11cb58edb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c088c57d27b489bb5e481c9c9edc87c",
              "IPY_MODEL_97537c08080d498b91d2596352b50ed1",
              "IPY_MODEL_f0bf5740b1414a5495d83f2ffd872d1f"
            ],
            "layout": "IPY_MODEL_b38f59b197b54933806cc5ade4f2ab34"
          }
        },
        "8c088c57d27b489bb5e481c9c9edc87c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ade354dddabf45c6a2fd500b502375b3",
            "placeholder": "​",
            "style": "IPY_MODEL_e3f3cbaf46dd416793b6047f74373705",
            "value": "model.safetensors: 100%"
          }
        },
        "97537c08080d498b91d2596352b50ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d9a7453066941e785562b45bbc7709b",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_303ceaa3321948fb86525fbddc72dcf7",
            "value": 440449768
          }
        },
        "f0bf5740b1414a5495d83f2ffd872d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e690558ad6ca48c99810b1952647d9c1",
            "placeholder": "​",
            "style": "IPY_MODEL_52cbcb7de1724e04acb0f2d1358f104d",
            "value": " 440M/440M [00:02&lt;00:00, 157MB/s]"
          }
        },
        "b38f59b197b54933806cc5ade4f2ab34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade354dddabf45c6a2fd500b502375b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f3cbaf46dd416793b6047f74373705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d9a7453066941e785562b45bbc7709b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "303ceaa3321948fb86525fbddc72dcf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e690558ad6ca48c99810b1952647d9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52cbcb7de1724e04acb0f2d1358f104d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "852720b39a234edc8c5d8a065e5bb30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e873319ad354da48ffe82a5bb6ab3d5",
              "IPY_MODEL_0d7da9cafe394f83a672993665dd5dec",
              "IPY_MODEL_e9e27cafc018403f97e34e4e923d3af4"
            ],
            "layout": "IPY_MODEL_6582f165cf204e2d82d2f27a37504a1d"
          }
        },
        "8e873319ad354da48ffe82a5bb6ab3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb64bfc91034fdba5301ed6268b6300",
            "placeholder": "​",
            "style": "IPY_MODEL_f882efede33e4e218d8cc7068daf2233",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0d7da9cafe394f83a672993665dd5dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_153a5c42800c44788236eca61cae2066",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a81c6a9057e742889f887087cb683cff",
            "value": 48
          }
        },
        "e9e27cafc018403f97e34e4e923d3af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b59e4119a9424aebbcf7c2fb2a5ea6dc",
            "placeholder": "​",
            "style": "IPY_MODEL_963d81926d9c4ed89a030600ec2c5055",
            "value": " 48.0/48.0 [00:00&lt;00:00, 6.17kB/s]"
          }
        },
        "6582f165cf204e2d82d2f27a37504a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beb64bfc91034fdba5301ed6268b6300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f882efede33e4e218d8cc7068daf2233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "153a5c42800c44788236eca61cae2066": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81c6a9057e742889f887087cb683cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b59e4119a9424aebbcf7c2fb2a5ea6dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963d81926d9c4ed89a030600ec2c5055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ebb93149af4437a9ed2d4e21af5514b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0fb6a65e7fe43f087c889be26c17c82",
              "IPY_MODEL_76f84555dcb8439a956d292872e2716d",
              "IPY_MODEL_3b9d22b06bd3466c9c29827428afd685"
            ],
            "layout": "IPY_MODEL_504b3c1e567945b4a3c764d871225106"
          }
        },
        "a0fb6a65e7fe43f087c889be26c17c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ec3a9af42044fb2b5295c772b9a313a",
            "placeholder": "​",
            "style": "IPY_MODEL_738f2bf6d5134aec96aa59357b7a52c3",
            "value": "vocab.txt: 100%"
          }
        },
        "76f84555dcb8439a956d292872e2716d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c7ff83c86c94e4dbde8e8877b2019aa",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72ba44a9c40342d7a0e1046b64885572",
            "value": 231508
          }
        },
        "3b9d22b06bd3466c9c29827428afd685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb64c4c2fc44344aa8b7838ee4c555a",
            "placeholder": "​",
            "style": "IPY_MODEL_44ebeecddbeb4847bc87604a7cccdc4a",
            "value": " 232k/232k [00:00&lt;00:00, 4.66MB/s]"
          }
        },
        "504b3c1e567945b4a3c764d871225106": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ec3a9af42044fb2b5295c772b9a313a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738f2bf6d5134aec96aa59357b7a52c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c7ff83c86c94e4dbde8e8877b2019aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ba44a9c40342d7a0e1046b64885572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4eb64c4c2fc44344aa8b7838ee4c555a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44ebeecddbeb4847bc87604a7cccdc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c525645561914d2591dbf045379c968d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bd797745d05410c9854ae765324950e",
              "IPY_MODEL_3cb145f032bb457fbafb96ca966b430a",
              "IPY_MODEL_80e2ecf323d440c6815b00344ee1c6df"
            ],
            "layout": "IPY_MODEL_408b686e859c4790b0aeddfa5d554c49"
          }
        },
        "8bd797745d05410c9854ae765324950e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f7c5d711edc424ab48d98cf151bb058",
            "placeholder": "​",
            "style": "IPY_MODEL_b4cc31af3faa427dab9c5b98f985b359",
            "value": "tokenizer.json: 100%"
          }
        },
        "3cb145f032bb457fbafb96ca966b430a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88624d2d259841f89390d5d4a72304c6",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1ab1236b7d843ceaca7c436a2cbaae7",
            "value": 466062
          }
        },
        "80e2ecf323d440c6815b00344ee1c6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d45cb47de73e4701892486d5d86d409e",
            "placeholder": "​",
            "style": "IPY_MODEL_1730cb30c2a7487aadbd43cbb41b91ef",
            "value": " 466k/466k [00:00&lt;00:00, 19.0MB/s]"
          }
        },
        "408b686e859c4790b0aeddfa5d554c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f7c5d711edc424ab48d98cf151bb058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4cc31af3faa427dab9c5b98f985b359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88624d2d259841f89390d5d4a72304c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1ab1236b7d843ceaca7c436a2cbaae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d45cb47de73e4701892486d5d86d409e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1730cb30c2a7487aadbd43cbb41b91ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## In this notebook, two subtask of the Lexical Simplification Pipeline will be implemented:\n",
        "\n",
        "1. Substitution Generation (the first method is Word2Vec, augmnted with POS filtering, and the second method is LSBert, which is based on Masking Language Models)\n",
        "2. Substitution Selection ( the first method is computing the content words' vector vs. each candidate vector to select the best generated candidate. The second method is asking a decoder only language model to select from the substitutes)\n",
        "\n",
        "For the task of Substitution Geberation, SG, neither trial nor training datasets will be used becuase the Word2vec and BERT are already pre-trained on millions of tokens.\n",
        "\n",
        "The MLSP2024 Dataset will be used for testing and evaluation."
      ],
      "metadata": {
        "id": "ju4BgwkrZpXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 1. **Substiution Generation**: having the target complex word selected, two methods will be used to generate candidates."
      ],
      "metadata": {
        "id": "DmU6ROPemr6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "##1.1 Word2Vec with POS filtering:\n",
        "an already pre-trained Word2vec model will be installed, navigating through all the selected complex words in the dataset MLSP with labels. After generation, the generated candidates will be evaluated against the human annotated candidates in the datatset.\n",
        "\n",
        "the metrics used are\n",
        "\n",
        "1. potential@10\n",
        "2. precesion@10\n",
        "3. Recall@10\n"
      ],
      "metadata": {
        "id": "DsGb-3xunNk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When running the first code block, the envirmomnet will ask the user to restart session. Please do and run the remaining codes."
      ],
      "metadata": {
        "id": "QR8qGFVMbAlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy pandas gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xrhGfsEo9rO7",
        "outputId": "8168669d-66c2-4ae4-9020-f0bfe18ef137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytz, wrapt, tzdata, six, numpy, smart-open, scipy, python-dateutil, pandas, gensim\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 scipy-1.13.1 six-1.17.0 smart-open-7.1.0 tzdata-2025.2 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              },
              "id": "804b741a1e61400d983b941e831cb924"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# loading the dataset of MLSP with labels to navigate through the candidates\n",
        "df = pd.read_csv(\"/content/multils_test_english_combined_labels.tsv\", sep=\"\\t\")\n",
        "\n",
        "# Subset the important columns\n",
        "df_subset = df[[\"context\", \"target\"] + [col for col in df.columns if col.startswith(\"substitution_\")]]\n",
        "\n",
        "# Preview\n",
        "df_subset.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "id": "STpVZNfYDQOZ",
        "outputId": "75668e73-755d-4ff2-9756-9092f4f75e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             context        target  \\\n",
              "0  After Ron nearly dies drinking poisoned mead t...    distraught   \n",
              "1  After Ron nearly dies drinking poisoned mead t...      drinking   \n",
              "2  After the war, Hitler remained in the army and...       oratory   \n",
              "3  After the war, Hitler remained in the army and...     reporting   \n",
              "4  After the war, Hitler remained in the army and...  infiltrating   \n",
              "\n",
              "  substitution_1 substitution_2 substitution_3 substitution_4 substitution_5  \\\n",
              "0        worried        worried       troubled       troubled     distressed   \n",
              "1      consuming      consuming      consuming      consuming         taking   \n",
              "2         speech         speech       speaking       speaking          vocal   \n",
              "3      notifying      notifying      notifying      informing      informing   \n",
              "4       invading       invading       invading    penetrating    penetrating   \n",
              "\n",
              "  substitution_6 substitution_7 substitution_8  ... substitution_21  \\\n",
              "0     distressed          upset      terrified  ...             NaN   \n",
              "1         taking       intaking      ingesting  ...             NaN   \n",
              "2           talk           oral            NaN  ...             NaN   \n",
              "3      informing        telling        telling  ...             NaN   \n",
              "4      intruding       entering            NaN  ...             NaN   \n",
              "\n",
              "  substitution_22 substitution_23 substitution_24 substitution_25  \\\n",
              "0             NaN             NaN             NaN             NaN   \n",
              "1             NaN             NaN             NaN             NaN   \n",
              "2             NaN             NaN             NaN             NaN   \n",
              "3             NaN             NaN             NaN             NaN   \n",
              "4             NaN             NaN             NaN             NaN   \n",
              "\n",
              "  substitution_26 substitution_27 substitution_28 substitution_29  \\\n",
              "0             NaN             NaN             NaN             NaN   \n",
              "1             NaN             NaN             NaN             NaN   \n",
              "2             NaN             NaN             NaN             NaN   \n",
              "3             NaN             NaN             NaN             NaN   \n",
              "4             NaN             NaN             NaN             NaN   \n",
              "\n",
              "   substitution_30  \n",
              "0              NaN  \n",
              "1              NaN  \n",
              "2              NaN  \n",
              "3              NaN  \n",
              "4              NaN  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83f248df-a209-447f-99bb-443d5018e451\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>target</th>\n",
              "      <th>substitution_1</th>\n",
              "      <th>substitution_2</th>\n",
              "      <th>substitution_3</th>\n",
              "      <th>substitution_4</th>\n",
              "      <th>substitution_5</th>\n",
              "      <th>substitution_6</th>\n",
              "      <th>substitution_7</th>\n",
              "      <th>substitution_8</th>\n",
              "      <th>...</th>\n",
              "      <th>substitution_21</th>\n",
              "      <th>substitution_22</th>\n",
              "      <th>substitution_23</th>\n",
              "      <th>substitution_24</th>\n",
              "      <th>substitution_25</th>\n",
              "      <th>substitution_26</th>\n",
              "      <th>substitution_27</th>\n",
              "      <th>substitution_28</th>\n",
              "      <th>substitution_29</th>\n",
              "      <th>substitution_30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>After Ron nearly dies drinking poisoned mead t...</td>\n",
              "      <td>distraught</td>\n",
              "      <td>worried</td>\n",
              "      <td>worried</td>\n",
              "      <td>troubled</td>\n",
              "      <td>troubled</td>\n",
              "      <td>distressed</td>\n",
              "      <td>distressed</td>\n",
              "      <td>upset</td>\n",
              "      <td>terrified</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>After Ron nearly dies drinking poisoned mead t...</td>\n",
              "      <td>drinking</td>\n",
              "      <td>consuming</td>\n",
              "      <td>consuming</td>\n",
              "      <td>consuming</td>\n",
              "      <td>consuming</td>\n",
              "      <td>taking</td>\n",
              "      <td>taking</td>\n",
              "      <td>intaking</td>\n",
              "      <td>ingesting</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>After the war, Hitler remained in the army and...</td>\n",
              "      <td>oratory</td>\n",
              "      <td>speech</td>\n",
              "      <td>speech</td>\n",
              "      <td>speaking</td>\n",
              "      <td>speaking</td>\n",
              "      <td>vocal</td>\n",
              "      <td>talk</td>\n",
              "      <td>oral</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>After the war, Hitler remained in the army and...</td>\n",
              "      <td>reporting</td>\n",
              "      <td>notifying</td>\n",
              "      <td>notifying</td>\n",
              "      <td>notifying</td>\n",
              "      <td>informing</td>\n",
              "      <td>informing</td>\n",
              "      <td>informing</td>\n",
              "      <td>telling</td>\n",
              "      <td>telling</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>After the war, Hitler remained in the army and...</td>\n",
              "      <td>infiltrating</td>\n",
              "      <td>invading</td>\n",
              "      <td>invading</td>\n",
              "      <td>invading</td>\n",
              "      <td>penetrating</td>\n",
              "      <td>penetrating</td>\n",
              "      <td>intruding</td>\n",
              "      <td>entering</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83f248df-a209-447f-99bb-443d5018e451')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-83f248df-a209-447f-99bb-443d5018e451 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-83f248df-a209-447f-99bb-443d5018e451');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b27b7acd-7f1b-4fab-a188-4a10352ca8d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b27b7acd-7f1b-4fab-a188-4a10352ca8d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b27b7acd-7f1b-4fab-a188-4a10352ca8d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_subset"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #loading this library for POS filtering\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from gensim.models import KeyedVectors #loading a pre-trained Word2Vec to generate candidates for the complex words\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXhfnrXsFHnf",
        "outputId": "14d98191-d18a-4f7b-ec48-b8b6c4eaa2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_substitutes(sentence, target_word, topn=10):\n",
        "    \"\"\"\n",
        "    Generates substitutes for a given target word in a sentence using Word2Vec\n",
        "    and POS filtering.\n",
        "\n",
        "    1) First parameter is the sentence, containing the target word\n",
        "    2) Second parameter is the target word\n",
        "    3) Third parameter is the number of substitutes to generate\n",
        "\n",
        "    Returns a list of substitutes that are semantically and syntactically\n",
        "    similar to the target word.\n",
        "    \"\"\"\n",
        "    words = word_tokenize(sentence) #the sentence is tokenized into words to find the POS of the target word\n",
        "    tags = pos_tag(words)\n",
        "\n",
        "    # Find POS of target word\n",
        "    target_pos = None\n",
        "    for word, tag in tags:\n",
        "        if word.lower() == target_word.lower():\n",
        "            target_pos = tag\n",
        "            break\n",
        "\n",
        "    # Get most similar words of the complex one by returning to Word2Vec\n",
        "    if target_word in w2v_model:\n",
        "        candidates = [w for w, _ in w2v_model.most_similar(target_word, topn=50)]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    # Finally, the candidates would run thorugh the POS filter.\n",
        "    filtered = []\n",
        "    for cand in candidates:\n",
        "        cand_tagged = pos_tag([cand])[0][1]\n",
        "        if cand_tagged == target_pos:\n",
        "            filtered.append(cand)\n",
        "        if len(filtered) == topn:\n",
        "            break\n",
        "\n",
        "    return filtered\n"
      ],
      "metadata": {
        "id": "UcrytidRFaqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying the function on a random sentence\n",
        "example = df_subset.iloc[80]\n",
        "sentence = example[\"context\"]\n",
        "target_word = example[\"target\"]\n",
        "\n",
        "subs = generate_substitutes(sentence, target_word)\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Target Word:\", target_word)\n",
        "print(\"Substitutes:\", subs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Cl1YGdFimt",
        "outputId": "39b2a5b2-3ee3-4661-cd44-fabf6d8a8f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Currently, one seat equals roughly 600,000 constituents.\n",
            "Target Word: equals\n",
            "Substitutes: ['equates', 'translates', 'means', 'begets', 'Equals', 'implies', 'constitutes', 'multiplies', 'trumps', 'exceeds']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1.2 Mask Language Model (MLM):\n",
        "\n",
        "In this approach, candidates will be generated using a Transformers-based Language Model, BERT. The complex word is hidden using a special token [MASK]. Based on the context of the sentence, the language model will use the attention mechansim to understand the context and the linguistic nuances of the sentence, thereby generating potential alternatives.\n",
        "\n",
        "However, this approach is quite different from the Word2Vec one since context is taken as granted.\n",
        "\n",
        "the metrics used are\n",
        "\n",
        "1. potential@10\n",
        "2. precesion@10\n",
        "3. Recall@10\n"
      ],
      "metadata": {
        "id": "f2Z3KOx1xDn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the MLM approach on one sentence with the complex word being masked\n",
        "\n",
        "!pip install -q transformers sentencepiece\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the fill-mask pipeline with BERT\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "# Example usage\n",
        "masked_sent = \"After Ron nearly dies drinking poisoned mead that was apparently intended for Professor Dumbledore, Hermione becomes so [MASK] that they end their feud for good.\"\n",
        "print(fill_mask(masked_sent))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "511f190358a145e6992b376e8d60831c",
            "e8c24199b8494166a8eeac4354e6d26f",
            "4b418f79b9d34bbc8e91711d10860aa0",
            "1d060af784604ce79972b9d9c83dcb47",
            "dde55c72988843ea8e09546dd1778348",
            "8f0bf5b4e22e4b46b1990863e5da9ec5",
            "b334563559a844c7aee0f2f95749243c",
            "a5d02c1e760b4a589106dd47dd72eada",
            "93cade46024c48758c5a3b5c8a7c0ad1",
            "a82694b37ec245048ef9dd5a3541cffc",
            "396687ee1d6248eda561684a94f64f01",
            "454af9db73804a1a80289b11cb58edb6",
            "8c088c57d27b489bb5e481c9c9edc87c",
            "97537c08080d498b91d2596352b50ed1",
            "f0bf5740b1414a5495d83f2ffd872d1f",
            "b38f59b197b54933806cc5ade4f2ab34",
            "ade354dddabf45c6a2fd500b502375b3",
            "e3f3cbaf46dd416793b6047f74373705",
            "1d9a7453066941e785562b45bbc7709b",
            "303ceaa3321948fb86525fbddc72dcf7",
            "e690558ad6ca48c99810b1952647d9c1",
            "52cbcb7de1724e04acb0f2d1358f104d",
            "852720b39a234edc8c5d8a065e5bb30c",
            "8e873319ad354da48ffe82a5bb6ab3d5",
            "0d7da9cafe394f83a672993665dd5dec",
            "e9e27cafc018403f97e34e4e923d3af4",
            "6582f165cf204e2d82d2f27a37504a1d",
            "beb64bfc91034fdba5301ed6268b6300",
            "f882efede33e4e218d8cc7068daf2233",
            "153a5c42800c44788236eca61cae2066",
            "a81c6a9057e742889f887087cb683cff",
            "b59e4119a9424aebbcf7c2fb2a5ea6dc",
            "963d81926d9c4ed89a030600ec2c5055",
            "0ebb93149af4437a9ed2d4e21af5514b",
            "a0fb6a65e7fe43f087c889be26c17c82",
            "76f84555dcb8439a956d292872e2716d",
            "3b9d22b06bd3466c9c29827428afd685",
            "504b3c1e567945b4a3c764d871225106",
            "1ec3a9af42044fb2b5295c772b9a313a",
            "738f2bf6d5134aec96aa59357b7a52c3",
            "5c7ff83c86c94e4dbde8e8877b2019aa",
            "72ba44a9c40342d7a0e1046b64885572",
            "4eb64c4c2fc44344aa8b7838ee4c555a",
            "44ebeecddbeb4847bc87604a7cccdc4a",
            "c525645561914d2591dbf045379c968d",
            "8bd797745d05410c9854ae765324950e",
            "3cb145f032bb457fbafb96ca966b430a",
            "80e2ecf323d440c6815b00344ee1c6df",
            "408b686e859c4790b0aeddfa5d554c49",
            "8f7c5d711edc424ab48d98cf151bb058",
            "b4cc31af3faa427dab9c5b98f985b359",
            "88624d2d259841f89390d5d4a72304c6",
            "c1ab1236b7d843ceaca7c436a2cbaae7",
            "d45cb47de73e4701892486d5d86d409e",
            "1730cb30c2a7487aadbd43cbb41b91ef"
          ]
        },
        "id": "RIaFp4GqKxBV",
        "outputId": "2fa553c9-d9bb-4ce1-b6ad-04838b7a7332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "511f190358a145e6992b376e8d60831c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "454af9db73804a1a80289b11cb58edb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "852720b39a234edc8c5d8a065e5bb30c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ebb93149af4437a9ed2d4e21af5514b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c525645561914d2591dbf045379c968d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.2317497581243515, 'token': 4854, 'token_str': 'angry', 'sequence': 'after ron nearly dies drinking poisoned mead that was apparently intended for professor dumbledore, hermione becomes so angry that they end their feud for good.'}, {'score': 0.17666611075401306, 'token': 18835, 'token_str': 'enraged', 'sequence': 'after ron nearly dies drinking poisoned mead that was apparently intended for professor dumbledore, hermione becomes so enraged that they end their feud for good.'}, {'score': 0.1215352937579155, 'token': 6314, 'token_str': 'upset', 'sequence': 'after ron nearly dies drinking poisoned mead that was apparently intended for professor dumbledore, hermione becomes so upset that they end their feud for good.'}, {'score': 0.0993843525648117, 'token': 9943, 'token_str': 'furious', 'sequence': 'after ron nearly dies drinking poisoned mead that was apparently intended for professor dumbledore, hermione becomes so furious that they end their feud for good.'}, {'score': 0.03759358450770378, 'token': 9981, 'token_str': 'jealous', 'sequence': 'after ron nearly dies drinking poisoned mead that was apparently intended for professor dumbledore, hermione becomes so jealous that they end their feud for good.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Set up the LSBert Masked Language Model pipeline\n",
        "mlm_pipeline = pipeline(\"fill-mask\", model=\"bert-base-uncased\", device=0)\n",
        "\n",
        "# Define the substitution function using LSBert\n",
        "def generate_lm_substitutes(sentence, target_word, topn=10):\n",
        "    \"\"\"\n",
        "    this function is used to generate context-sensitive substitutes for a given target word in a sentence using\n",
        "    the Masked Language Model (MLM) approach.\n",
        "\n",
        "    the function replaces the complex word with a [MASK] token, then, based on the context, the model,\n",
        "    in this case is BERT, predicts the top n substitutes that are pragmatically and syntactically similar to the target word.\n",
        "\n",
        "    1) First parameter is the sentence, containing the target word\n",
        "    2) Second parameter is the target word\n",
        "    3) Third parameter is the number of substitutes to generate\n",
        "\n",
        "    Returns a list of substitutes\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Replace the target word with the [MASK] token\n",
        "        masked_sentence = sentence.replace(target_word, \"[MASK]\", 1)\n",
        "\n",
        "        # Generate predictions\n",
        "        outputs = mlm_pipeline(masked_sentence, top_k=topn)\n",
        "\n",
        "        # Extract the top substitutes\n",
        "        substitutes = [out[\"token_str\"] for out in outputs]\n",
        "        return substitutes\n",
        "    except:\n",
        "        return []\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjQksfrxLmeg",
        "outputId": "cfb8801f-673a-4bda-cb1b-b1bab4dfbf01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "this code block loops through each row in the dataset and\n",
        "apply the two generating functions, one based on Word2vec and the second on Masking language model. Afterwards, the generated substitutes are stored into two new column called \"generated substitutes\" and \"mlm_substitutes\",\n",
        "respectively.\n",
        "This step comes to evaluate the generated substitutes against the human annotated substitutes\n",
        "in the dataset.\n",
        "\"\"\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Running all generation steps\")\n",
        "\n",
        "# Word2Vec + POS filtering SG\n",
        "if \"generated_substitutes\" not in df_subset.columns:\n",
        "    df_subset[\"generated_substitutes\"] = df_subset.progress_apply(\n",
        "        lambda row: generate_substitutes(row[\"context\"], row[\"target\"], topn=10),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "# MLM (like LSBert or masked BERT) SG\n",
        "if \"mlm_substitutes\" not in df_subset.columns:\n",
        "    df_subset[\"mlm_substitutes\"] = df_subset.progress_apply(\n",
        "        lambda row: generate_lm_substitutes(row[\"context\"], row[\"target\"]),\n",
        "        axis=1\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zms9tgW1BFHq",
        "outputId": "c6f48c2b-cdef-432d-df65-d8113b4127c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running all generation steps: 100%|██████████| 570/570 [01:32<00:00,  6.16it/s]\n",
            "<ipython-input-6-5e2da8aabb54>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_subset[\"generated_substitutes\"] = df_subset.progress_apply(\n",
            "Running all generation steps: 100%|██████████| 570/570 [00:42<00:00, 13.31it/s]\n",
            "<ipython-input-6-5e2da8aabb54>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_subset[\"mlm_substitutes\"] = df_subset.progress_apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# After the candidates have been generated using two different apporaches, each approach will be evaluated a specific testing function\n",
        "\n",
        "1. The Word2vec approach will be evaluated with a function called\n",
        "\n",
        "```\n",
        "def evaluate_predictions(df, k=10):\n",
        "```\n",
        "\n",
        "2. The MLM approach will be evaluated with a function called\n",
        "\n",
        "\n",
        "```\n",
        "def evaluate_mlm_predictions(df, k=10):\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mkIqGBA9GbTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_predictions(df, k=10):\n",
        "    \"\"\"\n",
        "    this function evaluates the generated substitutes against the human annotated substitutes\n",
        "    in the dataset.\n",
        "\n",
        "    it takes the dataset that contains the additional column containing the generated substitutes.\n",
        "    it also takes the number of top predicted substitutes to evaluate.\n",
        "\n",
        "    the function returns the metrics:\n",
        "    1. potential@10: at least one correct substitute appears in the top 10 predictions\n",
        "    2. precesion@10: Average number of correct substitutes found among the top-10 predictions, divided by 10\n",
        "    3. Recall@10: Average proportion of gold substitutes that appear in the top-10 predictions.\n",
        "    \"\"\"\n",
        "    potential_hits = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_instances = 0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        gold_subs = set(str(s).lower() for s in row[df.columns[2:-1]].dropna())\n",
        "        pred_subs = row[\"generated_substitutes\"][:k] if isinstance(row[\"generated_substitutes\"], list) else []\n",
        "\n",
        "        if not gold_subs or not pred_subs:\n",
        "            continue\n",
        "        total_instances += 1\n",
        "        gold_hit = len(set(pred_subs) & gold_subs)\n",
        "\n",
        "        # Metrics\n",
        "        potential_hits += int(gold_hit > 0)\n",
        "        total_precision += gold_hit / k\n",
        "        total_recall += gold_hit / len(gold_subs)\n",
        "\n",
        "    print(f\"\\n Evaluation Results (k={k}):\")\n",
        "    print(\"Potential@10:\", round(potential_hits / total_instances, 3))\n",
        "    print(\"Precision@10:\", round(total_precision / total_instances, 3))\n",
        "    print(\"Recall@10:\", round(total_recall / total_instances, 3))\n",
        "\n",
        "\n",
        "\n",
        "evaluate_predictions(df_subset, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBS94s-xEwbM",
        "outputId": "85619379-78c7-425a-f41c-f73c58720be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluation Results (k=10):\n",
            "Potential@10: 0.442\n",
            "Precision@10: 0.072\n",
            "Recall@10: 0.115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_mlm_predictions(df, k=10):\n",
        "    \"\"\"\n",
        "    this function evaluates the generated substitutes against the human annotated substitutes\n",
        "    in the dataset.\n",
        "\n",
        "    it takes the dataset that contains the additional column containing the generated substitutes.\n",
        "    it also takes the number of top predicted substitutes to evaluate.\n",
        "\n",
        "    the function returns the metrics:\n",
        "    1. potential@10: at least one correct substitute appears in the top 10 predictions\n",
        "    2. precesion@10: Average number of correct substitutes found among the top-10 predictions, divided by 10\n",
        "    3. Recall@10: Average proportion of gold substitutes that appear in the top-10 predictions.\n",
        "    \"\"\"\n",
        "    potential_hits = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_instances = 0\n",
        "\n",
        "    sub_cols = [col for col in df.columns if col.startswith(\"substitution_\")]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        gold_subs = set(str(s).lower() for s in row[sub_cols].dropna())\n",
        "        pred_subs = row[\"mlm_substitutes\"][:k] if isinstance(row[\"mlm_substitutes\"], list) else []\n",
        "\n",
        "        if not gold_subs or not pred_subs:\n",
        "            continue  # Skip if missing data\n",
        "\n",
        "        total_instances += 1\n",
        "        gold_hit = len(set(pred_subs) & gold_subs)\n",
        "\n",
        "        # Metrics\n",
        "        potential_hits += int(gold_hit > 0)\n",
        "        total_precision += gold_hit / k\n",
        "        total_recall += gold_hit / len(gold_subs)\n",
        "\n",
        "    print(f\"\\nEvaluation Results for MLM (k={k}):\")\n",
        "    print(\"Potential@10:\", round(potential_hits / total_instances, 3))\n",
        "    print(\"Precision@10:\", round(total_precision / total_instances, 3))\n",
        "    print(\"Recall@10:\", round(total_recall / total_instances, 3))\n",
        "\n",
        "# Run it\n",
        "evaluate_mlm_predictions(df_subset, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBlrkPouGRbX",
        "outputId": "6c9ba97e-f686-4b99-ef8d-b58053596a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results for MLM (k=10):\n",
            "Potential@10: 0.577\n",
            "Precision@10: 0.098\n",
            "Recall@10: 0.201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 2. Substitution Selection: Given that the candidate words are generated, two methods will be used to select the best candidate\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fEcEeMWzHGT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "# 2.1 (Paetzold & Specia 2015) Style Semantic Ranking:\n",
        "\n",
        "\n",
        "\n",
        "to select the best candidates, generated from Word2Vec method, the candidates are ranked based on their semantic similarity to the content words in the sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qn8cAaCH7Q8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download stopwords corpus to remove them from the sentence\n",
        "#only content words are ranked (no stopwords)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Grab proper English stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Paetzold & Specia (2015) style ranking function\n",
        "def rank_by_context_similarity(context, candidates):\n",
        "    tokens = word_tokenize(context.lower())\n",
        "    content_words = [w for w in tokens if w.isalpha() and w not in stop_words and w in w2v_model]\n",
        "    if not content_words:\n",
        "        return [] # function to tokenize, remove punctuation, stopwords, and words not found in W2V\n",
        "\n",
        "    content_vecs = [w2v_model[w] for w in content_words] #getting the Vectors of the Content Words\n",
        "    context_matrix = np.stack(content_vecs)\n",
        "\n",
        "    ranked = []\n",
        "    for cand in candidates:\n",
        "        if cand in w2v_model:\n",
        "            cand_vec = w2v_model[cand].reshape(1, -1)\n",
        "            sims = cosine_similarity(cand_vec, context_matrix)\n",
        "            avg_sim = np.mean(sims)\n",
        "            ranked.append((cand, avg_sim)) # getting the vector for each candidate, comparing the vectors with sentence context. then, takes the average similarity across all content words\n",
        "\n",
        "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [w for w, _ in ranked]\n",
        "\n",
        "# Apply it to the Word2Vec-generated substitutes\n",
        "df_subset[\"w2v_context_ranked_subs\"] = df_subset.apply(\n",
        "    lambda row: rank_by_context_similarity(row[\"context\"], row[\"generated_substitutes\"])\n",
        "    if isinstance(row[\"generated_substitutes\"], list) else [],\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YsBJM6OPqOR",
        "outputId": "e5ec6f16-17c8-4611-94af-c7820e3e352a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ranked_predictions_by_proportion(df, proportion=0.5, column=\"w2v_context_ranked_subs\"):\n",
        "    \"\"\"\n",
        "    Evaluate ranked Word2Vec-based SS using top 'proportion' of candidates.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with ranked substitutes in column.\n",
        "        proportion: Proportion (e.g., 0.5 means top 50%).\n",
        "        column: Column name of ranked substitutes.\n",
        "\n",
        "    Returns:\n",
        "        Prints Potential@p, Precision@p, Recall@p\n",
        "    \"\"\"\n",
        "    potential_hits = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_instances = 0\n",
        "\n",
        "    sub_cols = [col for col in df.columns if col.startswith(\"substitution_\")]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        gold_subs = set(str(s).lower() for s in row[sub_cols].dropna())\n",
        "        ranked_subs = row[column]\n",
        "\n",
        "        if not gold_subs or not isinstance(ranked_subs, list) or not ranked_subs:\n",
        "            continue\n",
        "\n",
        "        k = max(1, int(len(ranked_subs) * proportion))\n",
        "        selected_subs = ranked_subs[:k]\n",
        "\n",
        "        gold_hit = len(set(selected_subs) & gold_subs)\n",
        "\n",
        "        total_instances += 1\n",
        "        potential_hits += int(gold_hit > 0)\n",
        "        total_precision += gold_hit / k\n",
        "        total_recall += gold_hit / len(gold_subs)\n",
        "\n",
        "    print(f\"\\nEvaluation Results for W2V Context-Ranking (Top {int(proportion*100)}%):\")\n",
        "    print(\" Potential@p:\", round(potential_hits / total_instances, 3))\n",
        "    print(\" Precision@p:\", round(total_precision / total_instances, 3))\n",
        "    print(\" Recall@p:\", round(total_recall / total_instances, 3))\n",
        "\n",
        "# Usage example\n",
        "evaluate_ranked_predictions_by_proportion(df_subset, proportion=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEgFKh957Qc8",
        "outputId": "f6ca79eb-a024-4fd8-ecfc-0a249821f741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results for W2V Context-Ranking (Top 50%):\n",
            " Potential@p: 0.344\n",
            " Precision@p: 0.113\n",
            " Recall@p: 0.095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 2.2 Prompt Engineering:\n",
        "\n",
        "in this method, the substitutes, generated from MLM, are fed into a decoder only model (Mistral-Instruct 7B V.1). The language model decides what is the best replacement for the complex word, given the candidates list.\n",
        "\n",
        "the prompt is:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\"\"\"<s>[INST] Given the sentence:\n",
        "\n",
        "\"{context}\"\n",
        "\n",
        "What is the best replacement for the complex word \"{target}\" in this list?\n",
        "\n",
        "{candidate_str}\n",
        "\n",
        "Respond with one word only. Do not explain. [/INST]\n",
        "\"\"\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LOX_XapsUDVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNIRh5jNWTZI",
        "outputId": "e048ec9a-cd0d-4297-eb09-a3e29e66b12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azXknTZKe85O",
        "outputId": "e91603e4-f8ec-43f0-b330-d321ab0d3bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Lexical Simplification` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Lexical Simplification`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ae481556485f47f2b6545a9125f3e7cd",
            "66c931ca6a004e97ae130f9dcd69da5f",
            "9ee420d75eb64cdeaaa914bf9a45a215",
            "a84db6fdf3334a7cb04f8c0e750ce771",
            "d94cede4d3814705b64b0dc37a1d3495",
            "84d3b7708cd5499591c2329c73797b97",
            "fb23deebf6e34c76af66e90b628ac216",
            "545a39e5447c460a87526c9169e383ea",
            "85999f0c5e27429fafc1c6b24e70882b",
            "4fd017aa72c2497cbfbfd6e25003e317",
            "09c984b4185e46f4a53621dd8c221521"
          ]
        },
        "id": "J4Ss_5_cqAdL",
        "outputId": "eb9fbc14-6c17-4e45-8bd7-2569d26b02b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae481556485f47f2b6545a9125f3e7cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing a prompting function to select the best replacement"
      ],
      "metadata": {
        "id": "uo1sD_SBVCaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(context, target, candidates):\n",
        "    candidate_str = \", \".join(candidates)\n",
        "\n",
        "    return f\"\"\"<s>[INST] Given the sentence:\n",
        "\n",
        "\"{context}\"\n",
        "\n",
        "What is the best replacement for the complex word \"{target}\" in this list?\n",
        "\n",
        "{candidate_str}\n",
        "\n",
        "Respond with one word only. Do not explain. [/INST]\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "e79mOATvqsTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fine-tuning the hyperparameters of the model to give us the best output needed, i.e. only one word."
      ],
      "metadata": {
        "id": "WcE5SCkuVdLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_substitute(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=5,     # we just need one word\n",
        "        temperature=0.7,      # a bit of diversity\n",
        "        top_k=20,             # limit randomness\n",
        "        top_p=0.9,            # nucleus sampling\n",
        "        do_sample=True,       # allow sampling\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.split(\"\\n\")[-1].strip()\n"
      ],
      "metadata": {
        "id": "9UUSf6nnqtN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting the dataset into three batches to save time"
      ],
      "metadata": {
        "id": "in_H-CCxVrx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split points\n",
        "batch1 = df_subset.iloc[:190]     # Rows 0–189\n",
        "batch2 = df_subset.iloc[190:380]  # Rows 190–379\n",
        "batch3 = df_subset.iloc[380:]     # Rows 380–569\n"
      ],
      "metadata": {
        "id": "Ihnep2zbrLFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Mistral v1 SS - Batch 1\")\n",
        "\n",
        "batch1[\"mlm_selected_sub\"] = batch1.progress_apply(\n",
        "    lambda row: get_best_substitute(\n",
        "        build_prompt(row[\"context\"], row[\"target\"], row[\"mlm_substitutes\"])\n",
        "    ) if isinstance(row[\"mlm_substitutes\"], list) and row[\"mlm_substitutes\"] else None,\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROTeqHQFukv5",
        "outputId": "d9fa6cb8-eeda-422d-c143-85051691cb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Mistral v1 SS - Batch 1:   0%|          | 0/190 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   1%|          | 2/190 [00:11<18:24,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   2%|▏         | 3/190 [00:21<22:50,  7.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   2%|▏         | 4/190 [00:32<27:45,  8.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   3%|▎         | 5/190 [00:39<25:34,  8.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   3%|▎         | 6/190 [00:46<24:08,  7.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   4%|▎         | 7/190 [00:58<27:44,  9.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   4%|▍         | 8/190 [01:07<27:48,  9.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   5%|▍         | 9/190 [01:14<25:37,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   5%|▌         | 10/190 [01:21<24:06,  8.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   6%|▌         | 11/190 [01:33<27:14,  9.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   6%|▋         | 12/190 [01:42<27:16,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   7%|▋         | 13/190 [01:54<29:22,  9.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   7%|▋         | 14/190 [02:06<30:45, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   8%|▊         | 15/190 [02:15<29:34, 10.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   8%|▊         | 16/190 [02:24<28:43,  9.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   9%|▉         | 17/190 [02:34<28:05,  9.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:   9%|▉         | 18/190 [02:43<27:36,  9.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  10%|█         | 19/190 [02:52<27:08,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  11%|█         | 20/190 [02:59<24:50,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  11%|█         | 21/190 [03:11<27:10,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  12%|█▏        | 22/190 [03:21<26:46,  9.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  12%|█▏        | 23/190 [03:32<28:21, 10.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  13%|█▎        | 24/190 [03:42<27:29,  9.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  13%|█▎        | 25/190 [03:53<28:46, 10.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  14%|█▎        | 26/190 [04:03<27:43, 10.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  14%|█▍        | 27/190 [04:14<28:49, 10.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  15%|█▍        | 28/190 [04:24<27:37, 10.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  15%|█▌        | 29/190 [04:31<24:49,  9.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  16%|█▌        | 30/190 [04:42<26:36,  9.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  16%|█▋        | 31/190 [04:54<27:47, 10.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  17%|█▋        | 32/190 [05:03<26:42, 10.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  17%|█▋        | 33/190 [05:13<25:54,  9.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  18%|█▊        | 34/190 [05:24<27:06, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  18%|█▊        | 35/190 [05:34<26:06, 10.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  19%|█▉        | 36/190 [05:45<27:08, 10.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  19%|█▉        | 37/190 [05:55<26:02, 10.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  20%|██        | 38/190 [06:06<27:00, 10.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  21%|██        | 39/190 [06:18<27:34, 10.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  21%|██        | 40/190 [06:30<27:56, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  22%|██▏       | 41/190 [06:37<24:38,  9.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  22%|██▏       | 42/190 [06:44<22:18,  9.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  23%|██▎       | 43/190 [06:55<24:04,  9.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  23%|██▎       | 44/190 [07:02<21:51,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  24%|██▎       | 45/190 [07:14<23:39,  9.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  24%|██▍       | 46/190 [07:23<23:09,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  25%|██▍       | 47/190 [07:30<21:06,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  25%|██▌       | 48/190 [07:40<21:18,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  26%|██▌       | 49/190 [07:49<21:23,  9.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  26%|██▋       | 50/190 [07:56<19:45,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  27%|██▋       | 51/190 [08:05<20:13,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  27%|██▋       | 52/190 [08:15<20:30,  8.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  28%|██▊       | 53/190 [08:27<22:16,  9.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  28%|██▊       | 54/190 [08:34<20:15,  8.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  29%|██▉       | 55/190 [08:43<20:21,  9.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  29%|██▉       | 56/190 [08:50<18:52,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  30%|███       | 57/190 [08:57<17:46,  8.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  31%|███       | 58/190 [09:09<20:03,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  31%|███       | 59/190 [09:16<18:32,  8.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  32%|███▏      | 60/190 [09:25<18:56,  8.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  32%|███▏      | 61/190 [09:37<20:40,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  33%|███▎      | 62/190 [09:46<20:18,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  33%|███▎      | 63/190 [09:58<21:30, 10.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  34%|███▎      | 64/190 [10:07<20:48,  9.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  34%|███▍      | 65/190 [10:19<21:45, 10.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  35%|███▍      | 66/190 [10:30<22:21, 10.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  35%|███▌      | 67/190 [10:42<22:43, 11.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  36%|███▌      | 68/190 [10:54<22:52, 11.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  36%|███▋      | 69/190 [11:03<21:32, 10.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  37%|███▋      | 70/190 [11:12<20:32, 10.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  37%|███▋      | 71/190 [11:19<18:25,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  38%|███▊      | 72/190 [11:31<19:40, 10.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  38%|███▊      | 73/190 [11:43<20:30, 10.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  39%|███▉      | 74/190 [11:50<18:17,  9.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  39%|███▉      | 75/190 [12:01<19:24, 10.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  40%|████      | 76/190 [12:08<17:27,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  41%|████      | 77/190 [12:15<16:03,  8.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  41%|████      | 78/190 [12:25<16:23,  8.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  42%|████▏     | 79/190 [12:34<16:35,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  42%|████▏     | 80/190 [12:43<16:38,  9.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  43%|████▎     | 81/190 [12:55<17:53,  9.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  43%|████▎     | 82/190 [13:02<16:12,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  44%|████▎     | 83/190 [13:09<14:59,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  44%|████▍     | 84/190 [13:21<16:35,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  45%|████▍     | 85/190 [13:32<17:37, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  45%|████▌     | 86/190 [13:42<17:05,  9.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  46%|████▌     | 87/190 [13:49<15:28,  9.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  46%|████▋     | 88/190 [13:58<15:29,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  47%|████▋     | 89/190 [14:10<16:36,  9.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  47%|████▋     | 90/190 [14:17<15:01,  9.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  48%|████▊     | 91/190 [14:26<15:02,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  48%|████▊     | 92/190 [14:33<13:51,  8.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  49%|████▉     | 93/190 [14:40<13:00,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  49%|████▉     | 94/190 [14:50<13:29,  8.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  50%|█████     | 95/190 [14:59<13:47,  8.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  51%|█████     | 96/190 [15:08<13:55,  8.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  51%|█████     | 97/190 [15:20<15:04,  9.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  52%|█████▏    | 98/190 [15:29<14:44,  9.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  52%|█████▏    | 99/190 [15:39<14:27,  9.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  53%|█████▎    | 100/190 [15:43<12:07,  8.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  53%|█████▎    | 101/190 [15:48<10:27,  7.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  54%|█████▎    | 102/190 [15:57<11:21,  7.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  54%|█████▍    | 103/190 [16:04<10:54,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  55%|█████▍    | 104/190 [16:14<11:33,  8.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  55%|█████▌    | 105/190 [16:18<09:59,  7.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  56%|█████▌    | 106/190 [16:28<10:51,  7.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  56%|█████▋    | 107/190 [16:37<11:22,  8.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  57%|█████▋    | 108/190 [16:44<10:43,  7.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  57%|█████▋    | 109/190 [16:56<12:09,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  58%|█████▊    | 110/190 [17:05<12:09,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  58%|█████▊    | 111/190 [17:15<12:07,  9.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  59%|█████▉    | 112/190 [17:26<12:55,  9.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  59%|█████▉    | 113/190 [17:38<13:25, 10.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  60%|██████    | 114/190 [17:47<12:48, 10.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  61%|██████    | 115/190 [17:54<11:28,  9.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  61%|██████    | 116/190 [18:04<11:22,  9.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  62%|██████▏   | 117/190 [18:15<12:07,  9.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  62%|██████▏   | 118/190 [18:25<11:44,  9.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  63%|██████▎   | 119/190 [18:36<12:16, 10.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  63%|██████▎   | 120/190 [18:46<11:44, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  64%|██████▎   | 121/190 [18:55<11:19,  9.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  64%|██████▍   | 122/190 [19:04<10:59,  9.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  65%|██████▍   | 123/190 [19:14<10:43,  9.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  65%|██████▌   | 124/190 [19:21<09:42,  8.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  66%|██████▌   | 125/190 [19:30<09:44,  8.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  66%|██████▋   | 126/190 [19:42<10:27,  9.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  67%|██████▋   | 127/190 [19:51<10:08,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  67%|██████▋   | 128/190 [20:03<10:36, 10.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  68%|██████▊   | 129/190 [20:10<09:26,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  68%|██████▊   | 130/190 [20:19<09:19,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  69%|██████▉   | 131/190 [20:31<09:51, 10.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  69%|██████▉   | 132/190 [20:40<09:30,  9.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  70%|███████   | 133/190 [20:47<08:32,  8.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  71%|███████   | 134/190 [20:54<07:50,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  71%|███████   | 135/190 [21:04<07:57,  8.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  72%|███████▏  | 136/190 [21:13<07:59,  8.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  72%|███████▏  | 137/190 [21:25<08:35,  9.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  73%|███████▎  | 138/190 [21:36<08:56, 10.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  73%|███████▎  | 139/190 [21:48<09:06, 10.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  74%|███████▎  | 140/190 [21:57<08:35, 10.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  74%|███████▍  | 141/190 [22:09<08:44, 10.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  75%|███████▍  | 142/190 [22:21<08:47, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  75%|███████▌  | 143/190 [22:30<08:13, 10.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  76%|███████▌  | 144/190 [22:42<08:20, 10.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  76%|███████▋  | 145/190 [22:49<07:16,  9.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  77%|███████▋  | 146/190 [23:01<07:33, 10.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  77%|███████▋  | 147/190 [23:12<07:40, 10.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  78%|███████▊  | 148/190 [23:19<06:43,  9.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  78%|███████▊  | 149/190 [23:31<06:59, 10.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  79%|███████▉  | 150/190 [23:40<06:38,  9.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  79%|███████▉  | 151/190 [23:50<06:21,  9.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  80%|████████  | 152/190 [23:59<06:07,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  81%|████████  | 153/190 [24:11<06:19, 10.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  81%|████████  | 154/190 [24:22<06:25, 10.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  82%|████████▏ | 155/190 [24:32<06:00, 10.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  82%|████████▏ | 156/190 [24:43<06:04, 10.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  83%|████████▎ | 157/190 [24:55<06:03, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  83%|████████▎ | 158/190 [25:05<05:36, 10.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  84%|████████▎ | 159/190 [25:14<05:15, 10.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  84%|████████▍ | 160/190 [25:26<05:18, 10.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  85%|████████▍ | 161/190 [25:35<04:57, 10.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  85%|████████▌ | 162/190 [25:44<04:39,  9.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  86%|████████▌ | 163/190 [25:56<04:42, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  86%|████████▋ | 164/190 [26:05<04:23, 10.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  87%|████████▋ | 165/190 [26:17<04:24, 10.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  87%|████████▋ | 166/190 [26:26<04:05, 10.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  88%|████████▊ | 167/190 [26:33<03:32,  9.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  88%|████████▊ | 168/190 [26:40<03:08,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  89%|████████▉ | 169/190 [26:50<03:04,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  89%|████████▉ | 170/190 [26:59<02:59,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  90%|█████████ | 171/190 [27:08<02:52,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  91%|█████████ | 172/190 [27:20<02:57,  9.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  91%|█████████ | 173/190 [27:29<02:44,  9.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  92%|█████████▏| 174/190 [27:39<02:33,  9.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  92%|█████████▏| 175/190 [27:48<02:22,  9.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  93%|█████████▎| 176/190 [27:57<02:12,  9.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  93%|█████████▎| 177/190 [28:04<01:53,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  94%|█████████▎| 178/190 [28:14<01:46,  8.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  94%|█████████▍| 179/190 [28:25<01:47,  9.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  95%|█████████▍| 180/190 [28:35<01:36,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  95%|█████████▌| 181/190 [28:46<01:32, 10.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  96%|█████████▌| 182/190 [28:58<01:25, 10.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  96%|█████████▋| 183/190 [29:03<01:02,  8.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  97%|█████████▋| 184/190 [29:14<00:58,  9.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  97%|█████████▋| 185/190 [29:24<00:47,  9.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  98%|█████████▊| 186/190 [29:35<00:40, 10.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  98%|█████████▊| 187/190 [29:47<00:31, 10.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  99%|█████████▉| 188/190 [29:59<00:21, 10.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1:  99%|█████████▉| 189/190 [30:06<00:09,  9.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1: 100%|██████████| 190/190 [30:17<00:00, 10.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 1: 100%|██████████| 190/190 [30:22<00:00,  9.59s/it]\n",
            "<ipython-input-64-40d3ccf6f64a>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  batch1[\"mlm_selected_sub\"] = batch1.progress_apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch1.to_csv(\"mistral_batch1.tsv\", sep=\"\\t\", index=False)\n",
        "from google.colab import files\n",
        "files.download(\"mistral_batch1.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BL4U4xFzvDE7",
        "outputId": "651a770d-2c4f-49fe-8dd6-5fefcb510d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d7cd5fcb-b11e-4fd0-b121-fb6d2156ac6a\", \"mistral_batch1.tsv\", 112328)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Mistral v1 SS - Batch 2\")\n",
        "\n",
        "batch2[\"mlm_selected_sub\"] = batch2.progress_apply(\n",
        "    lambda row: get_best_substitute(\n",
        "        build_prompt(row[\"context\"], row[\"target\"], row[\"mlm_substitutes\"])\n",
        "    ) if isinstance(row[\"mlm_substitutes\"], list) and row[\"mlm_substitutes\"] else None,\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-3F1KMC3Hta",
        "outputId": "7dadaa7e-88ba-4b31-a997-0f21641f3102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Mistral v1 SS - Batch 2:   0%|          | 0/190 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   1%|          | 2/190 [00:04<07:20,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   2%|▏         | 3/190 [00:16<19:27,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   2%|▏         | 4/190 [00:23<20:15,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   3%|▎         | 5/190 [00:32<23:07,  7.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   3%|▎         | 6/190 [00:39<22:29,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   4%|▎         | 7/190 [00:46<22:03,  7.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   4%|▍         | 8/190 [00:56<23:57,  7.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   5%|▍         | 9/190 [01:05<25:11,  8.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   5%|▌         | 10/190 [01:17<28:06,  9.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   6%|▌         | 11/190 [01:24<25:47,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   6%|▋         | 12/190 [01:33<26:16,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   7%|▋         | 13/190 [01:42<26:33,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   7%|▋         | 14/190 [01:52<26:40,  9.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   8%|▊         | 15/190 [02:01<26:44,  9.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   8%|▊         | 16/190 [02:08<24:40,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   9%|▉         | 17/190 [02:17<25:14,  8.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:   9%|▉         | 18/190 [02:27<25:36,  8.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  10%|█         | 19/190 [02:34<23:47,  8.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  11%|█         | 20/190 [02:41<22:31,  7.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  11%|█         | 21/190 [02:48<21:38,  7.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  12%|█▏        | 22/190 [02:59<24:53,  8.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  12%|█▏        | 23/190 [03:11<27:00,  9.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  13%|█▎        | 24/190 [03:23<28:27, 10.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  13%|█▎        | 25/190 [03:32<27:30, 10.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  14%|█▎        | 26/190 [03:41<26:49,  9.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  14%|█▍        | 27/190 [03:48<24:23,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  15%|█▍        | 28/190 [03:58<24:33,  9.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  15%|█▌        | 29/190 [04:05<22:43,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  16%|█▌        | 30/190 [04:14<23:17,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  16%|█▋        | 31/190 [04:26<25:29,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  17%|█▋        | 32/190 [04:33<23:14,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  17%|█▋        | 33/190 [04:44<25:19,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  18%|█▊        | 34/190 [04:54<24:54,  9.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  18%|█▊        | 35/190 [05:03<24:32,  9.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  19%|█▉        | 36/190 [05:12<24:14,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  19%|█▉        | 37/190 [05:22<23:59,  9.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  20%|██        | 38/190 [05:29<22:01,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  21%|██        | 39/190 [05:40<24:06,  9.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  21%|██        | 40/190 [05:52<25:33, 10.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  22%|██▏       | 41/190 [06:01<24:44,  9.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  22%|██▏       | 42/190 [06:13<25:51, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  23%|██▎       | 43/190 [06:20<23:07,  9.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  23%|██▎       | 44/190 [06:32<24:36, 10.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  24%|██▎       | 45/190 [06:44<25:34, 10.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  24%|██▍       | 46/190 [06:55<26:09, 10.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  25%|██▍       | 47/190 [07:02<23:11,  9.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  25%|██▌       | 48/190 [07:12<22:45,  9.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  26%|██▌       | 49/190 [07:19<20:45,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  26%|██▋       | 50/190 [07:26<19:20,  8.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  27%|██▋       | 51/190 [07:37<21:32,  9.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  27%|██▋       | 52/190 [07:47<21:24,  9.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  28%|██▊       | 53/190 [07:58<22:53, 10.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  28%|██▊       | 54/190 [08:08<22:14,  9.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  29%|██▉       | 55/190 [08:17<21:44,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  29%|██▉       | 56/190 [08:26<21:22,  9.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  30%|███       | 57/190 [08:33<19:31,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  31%|███       | 58/190 [08:43<19:44,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  31%|███       | 59/190 [08:54<21:24,  9.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  32%|███▏      | 60/190 [09:01<19:26,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  32%|███▏      | 61/190 [09:11<19:31,  9.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  33%|███▎      | 62/190 [09:22<21:02,  9.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  33%|███▎      | 63/190 [09:29<19:03,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  34%|███▎      | 64/190 [09:36<17:38,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  34%|███▍      | 65/190 [09:41<15:11,  7.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  35%|███▍      | 66/190 [09:51<16:21,  7.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  35%|███▌      | 67/190 [09:58<15:40,  7.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  36%|███▌      | 68/190 [10:05<15:09,  7.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  36%|███▋      | 69/190 [10:16<17:35,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  37%|███▋      | 70/190 [10:28<19:14,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  37%|███▋      | 71/190 [10:35<17:31,  8.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  38%|███▊      | 72/190 [10:44<17:42,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  38%|███▊      | 73/190 [10:54<17:49,  9.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  39%|███▉      | 74/190 [11:03<17:46,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  39%|███▉      | 75/190 [11:12<17:41,  9.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  40%|████      | 76/190 [11:24<18:55,  9.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  41%|████      | 77/190 [11:33<18:24,  9.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  41%|████      | 78/190 [11:45<19:19, 10.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  42%|████▏     | 79/190 [11:54<18:33, 10.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  42%|████▏     | 80/190 [12:04<18:02,  9.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  43%|████▎     | 81/190 [12:11<16:21,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  43%|████▎     | 82/190 [12:16<13:52,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  44%|████▎     | 83/190 [12:23<13:23,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  44%|████▍     | 84/190 [12:32<14:16,  8.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  45%|████▍     | 85/190 [12:39<13:34,  7.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  45%|████▌     | 86/190 [12:51<15:31,  8.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  46%|████▌     | 87/190 [13:02<16:46,  9.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  46%|████▋     | 88/190 [13:12<16:25,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  47%|████▋     | 89/190 [13:21<16:06,  9.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  47%|████▋     | 90/190 [13:33<17:01, 10.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  48%|████▊     | 91/190 [13:40<15:17,  9.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  48%|████▊     | 92/190 [13:47<14:02,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  49%|████▉     | 93/190 [13:56<14:16,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  49%|████▉     | 94/190 [14:06<14:22,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  50%|█████     | 95/190 [14:17<15:31,  9.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  51%|█████     | 96/190 [14:27<15:07,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  51%|█████     | 97/190 [14:38<15:53, 10.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  52%|█████▏    | 98/190 [14:50<16:23, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  52%|█████▏    | 99/190 [14:59<15:35, 10.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  53%|█████▎    | 100/190 [15:09<14:59,  9.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  53%|█████▎    | 101/190 [15:18<14:31,  9.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  54%|█████▎    | 102/190 [15:27<14:10,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  54%|█████▍    | 103/190 [15:34<12:51,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  55%|█████▍    | 104/190 [15:44<12:55,  9.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  55%|█████▌    | 105/190 [15:55<13:54,  9.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  56%|█████▌    | 106/190 [16:05<13:32,  9.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  56%|█████▋    | 107/190 [16:16<14:12, 10.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  57%|█████▋    | 108/190 [16:26<13:39,  9.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  57%|█████▋    | 109/190 [16:33<12:17,  9.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  58%|█████▊    | 110/190 [16:38<10:22,  7.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  58%|█████▊    | 111/190 [16:47<10:51,  8.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  59%|█████▉    | 112/190 [16:54<10:13,  7.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  59%|█████▉    | 113/190 [17:06<11:34,  9.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  60%|██████    | 114/190 [17:15<11:32,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  61%|██████    | 115/190 [17:24<11:29,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  61%|██████    | 116/190 [17:34<11:23,  9.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  62%|██████▏   | 117/190 [17:43<11:15,  9.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  62%|██████▏   | 118/190 [17:50<10:17,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  63%|██████▎   | 119/190 [17:55<08:46,  7.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  63%|██████▎   | 120/190 [18:04<09:20,  8.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  64%|██████▎   | 121/190 [18:13<09:41,  8.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  64%|██████▍   | 122/190 [18:23<09:51,  8.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  65%|██████▍   | 123/190 [18:27<08:22,  7.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  65%|██████▌   | 124/190 [18:34<08:04,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  66%|██████▌   | 125/190 [18:46<09:22,  8.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  66%|██████▋   | 126/190 [18:58<10:11,  9.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  67%|██████▋   | 127/190 [19:07<09:57,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  67%|██████▋   | 128/190 [19:14<09:02,  8.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  68%|██████▊   | 129/190 [19:21<08:22,  8.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  68%|██████▊   | 130/190 [19:31<08:33,  8.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  69%|██████▉   | 131/190 [19:42<09:20,  9.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  69%|██████▉   | 132/190 [19:52<09:08,  9.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  70%|███████   | 133/190 [20:01<08:57,  9.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  71%|███████   | 134/190 [20:10<08:46,  9.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  71%|███████   | 135/190 [20:17<07:57,  8.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  72%|███████▏  | 136/190 [20:27<07:59,  8.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  72%|███████▏  | 137/190 [20:38<08:35,  9.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  73%|███████▎  | 138/190 [20:48<08:20,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  73%|███████▎  | 139/190 [20:57<08:06,  9.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  74%|███████▎  | 140/190 [21:06<07:53,  9.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  74%|███████▍  | 141/190 [21:18<08:16, 10.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  75%|███████▍  | 142/190 [21:25<07:21,  9.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  75%|███████▌  | 143/190 [21:37<07:47,  9.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  76%|███████▌  | 144/190 [21:46<07:29,  9.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  76%|███████▋  | 145/190 [21:53<06:42,  8.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  77%|███████▋  | 146/190 [22:03<06:39,  9.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  77%|███████▋  | 147/190 [22:14<07:04,  9.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  78%|███████▊  | 148/190 [22:21<06:18,  9.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  78%|███████▊  | 149/190 [22:31<06:14,  9.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  79%|███████▉  | 150/190 [22:40<06:08,  9.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  79%|███████▉  | 151/190 [22:49<06:01,  9.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  80%|████████  | 152/190 [22:56<05:26,  8.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  81%|████████  | 153/190 [23:04<05:00,  8.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  81%|████████  | 154/190 [23:13<05:05,  8.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  82%|████████▏ | 155/190 [23:20<04:41,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  82%|████████▏ | 156/190 [23:29<04:47,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  83%|████████▎ | 157/190 [23:39<04:48,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  83%|████████▎ | 158/190 [23:48<04:45,  8.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  84%|████████▎ | 159/190 [23:55<04:18,  8.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  84%|████████▍ | 160/190 [24:04<04:19,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  85%|████████▍ | 161/190 [24:16<04:38,  9.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  85%|████████▌ | 162/190 [24:26<04:26,  9.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  86%|████████▌ | 163/190 [24:33<03:56,  8.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  86%|████████▋ | 164/190 [24:40<03:34,  8.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  87%|████████▋ | 165/190 [24:51<03:52,  9.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  87%|████████▋ | 166/190 [25:01<03:43,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  88%|████████▊ | 167/190 [25:08<03:18,  8.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  88%|████████▊ | 168/190 [25:19<03:30,  9.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  89%|████████▉ | 169/190 [25:31<03:34, 10.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  89%|████████▉ | 170/190 [25:38<03:05,  9.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  90%|█████████ | 171/190 [25:45<02:43,  8.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  91%|█████████ | 172/190 [25:55<02:38,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  91%|█████████ | 173/190 [26:04<02:32,  8.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  92%|█████████▏| 174/190 [26:16<02:37,  9.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  92%|█████████▏| 175/190 [26:25<02:25,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  93%|█████████▎| 176/190 [26:34<02:14,  9.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  93%|█████████▎| 177/190 [26:42<01:54,  8.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  94%|█████████▎| 178/190 [26:51<01:48,  9.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  94%|█████████▍| 179/190 [27:00<01:40,  9.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  95%|█████████▍| 180/190 [27:12<01:39,  9.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  95%|█████████▌| 181/190 [27:24<01:34, 10.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  96%|█████████▌| 182/190 [27:36<01:26, 10.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  96%|█████████▋| 183/190 [27:40<01:03,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  97%|█████████▋| 184/190 [27:45<00:46,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  97%|█████████▋| 185/190 [27:52<00:37,  7.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  98%|█████████▊| 186/190 [27:59<00:29,  7.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  98%|█████████▊| 187/190 [28:06<00:21,  7.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  99%|█████████▉| 188/190 [28:15<00:15,  7.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2:  99%|█████████▉| 189/190 [28:22<00:07,  7.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2: 100%|██████████| 190/190 [28:29<00:00,  7.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 2: 100%|██████████| 190/190 [28:41<00:00,  9.06s/it]\n",
            "<ipython-input-66-c359a4fe518c>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  batch2[\"mlm_selected_sub\"] = batch2.progress_apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch2.to_csv(\"mistral_batch2.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"mistral_batch2.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j6CWl8f83IPC",
        "outputId": "4c7b0471-ed36-4d26-bd2c-c40d7e3e6242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_07464019-0579-4329-aedb-602c546a88bb\", \"mistral_batch2.tsv\", 111621)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Mistral v1 SS - Batch 3\")\n",
        "\n",
        "batch3[\"mlm_selected_sub\"] = batch3.progress_apply(\n",
        "    lambda row: get_best_substitute(\n",
        "        build_prompt(row[\"context\"], row[\"target\"], row[\"mlm_substitutes\"])\n",
        "    ) if isinstance(row[\"mlm_substitutes\"], list) and row[\"mlm_substitutes\"] else None,\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mbEyJM59u9y",
        "outputId": "e7d04dd1-af0f-44f5-9367-7641e04b1c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Mistral v1 SS - Batch 3:   0%|          | 0/190 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   1%|          | 2/190 [00:09<14:53,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   2%|▏         | 3/190 [00:21<24:04,  7.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   2%|▏         | 4/190 [00:30<26:01,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   3%|▎         | 5/190 [00:37<24:28,  7.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   3%|▎         | 6/190 [00:45<23:28,  7.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   4%|▎         | 7/190 [00:52<22:44,  7.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   4%|▍         | 8/190 [00:59<22:16,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   5%|▍         | 9/190 [01:06<21:54,  7.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   5%|▌         | 10/190 [01:15<23:42,  7.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   6%|▌         | 11/190 [01:25<24:57,  8.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   6%|▋         | 12/190 [01:32<23:38,  7.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   7%|▋         | 13/190 [01:41<24:46,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   7%|▋         | 14/190 [01:50<25:30,  8.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   8%|▊         | 15/190 [02:02<28:02,  9.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   8%|▊         | 16/190 [02:11<27:39,  9.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   9%|▉         | 17/190 [02:21<27:20,  9.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:   9%|▉         | 18/190 [02:30<27:03,  9.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  10%|█         | 19/190 [02:40<26:50,  9.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  11%|█         | 20/190 [02:51<28:36, 10.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  11%|█         | 21/190 [03:03<29:45, 10.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  12%|█▏        | 22/190 [03:12<28:34, 10.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  12%|█▏        | 23/190 [03:22<27:45,  9.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  13%|█▎        | 24/190 [03:31<27:03,  9.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  13%|█▎        | 25/190 [03:43<28:28, 10.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  14%|█▎        | 26/190 [03:50<25:34,  9.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  14%|█▍        | 27/190 [03:57<23:33,  8.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  15%|█▍        | 28/190 [04:04<22:03,  8.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  15%|█▌        | 29/190 [04:11<21:00,  7.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  16%|█▌        | 30/190 [04:20<22:07,  8.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  16%|█▋        | 31/190 [04:27<20:56,  7.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  17%|█▋        | 32/190 [04:37<21:59,  8.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  17%|█▋        | 33/190 [04:48<24:31,  9.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  18%|█▊        | 34/190 [05:00<26:11, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  18%|█▊        | 35/190 [05:12<27:18, 10.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  19%|█▉        | 36/190 [05:24<28:01, 10.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  19%|█▉        | 37/190 [05:35<28:28, 11.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  20%|██        | 38/190 [05:45<26:54, 10.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  21%|██        | 39/190 [05:56<27:34, 10.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  21%|██        | 40/190 [06:08<27:56, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  22%|██▏       | 41/190 [06:17<26:25, 10.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  22%|██▏       | 42/190 [06:24<23:34,  9.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  23%|██▎       | 43/190 [06:36<24:59, 10.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  23%|██▎       | 44/190 [06:46<24:11,  9.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  24%|██▎       | 45/190 [06:55<23:36,  9.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  24%|██▍       | 46/190 [07:04<23:06,  9.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  25%|██▍       | 47/190 [07:11<21:06,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  25%|██▌       | 48/190 [07:18<19:39,  8.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  26%|██▌       | 49/190 [07:30<21:55,  9.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  26%|██▋       | 50/190 [07:37<20:09,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  27%|██▋       | 51/190 [07:44<18:51,  8.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  27%|██▋       | 52/190 [07:53<19:34,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  28%|██▊       | 53/190 [08:05<21:36,  9.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  28%|██▊       | 54/190 [08:14<21:21,  9.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  29%|██▉       | 55/190 [08:21<19:34,  8.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  29%|██▉       | 56/190 [08:33<21:24,  9.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  30%|███       | 57/190 [08:45<22:38, 10.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  31%|███       | 58/190 [08:56<23:26, 10.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  31%|███       | 59/190 [09:03<20:52,  9.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  32%|███▏      | 60/190 [09:13<20:36,  9.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  32%|███▏      | 61/190 [09:20<18:50,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  33%|███▎      | 62/190 [09:27<17:34,  8.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  33%|███▎      | 63/190 [09:34<16:42,  7.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  34%|███▎      | 64/190 [09:41<16:00,  7.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  34%|███▍      | 65/190 [09:50<16:58,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  35%|███▍      | 66/190 [10:00<17:35,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  35%|███▌      | 67/190 [10:04<15:05,  7.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  36%|███▌      | 68/190 [10:14<16:12,  7.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  36%|███▋      | 69/190 [10:25<18:19,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  37%|███▋      | 70/190 [10:37<19:44,  9.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  37%|███▋      | 71/190 [10:46<19:15,  9.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  38%|███▊      | 72/190 [10:56<18:52,  9.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  38%|███▊      | 73/190 [11:05<18:34,  9.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  39%|███▉      | 74/190 [11:15<18:20,  9.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  39%|███▉      | 75/190 [11:24<18:06,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  40%|████      | 76/190 [11:36<19:14, 10.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  41%|████      | 77/190 [11:45<18:37,  9.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  41%|████      | 78/190 [11:52<16:51,  9.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  42%|████▏     | 79/190 [12:04<18:10,  9.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  42%|████▏     | 80/190 [12:15<19:00, 10.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  43%|████▎     | 81/190 [12:27<19:33, 10.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  43%|████▎     | 82/190 [12:36<18:34, 10.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  44%|████▎     | 83/190 [12:46<17:54, 10.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  44%|████▍     | 84/190 [12:57<18:36, 10.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  45%|████▍     | 85/190 [13:04<16:35,  9.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  45%|████▌     | 86/190 [13:11<15:09,  8.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  46%|████▌     | 87/190 [13:18<14:07,  8.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  46%|████▋     | 88/190 [13:28<14:32,  8.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  47%|████▋     | 89/190 [13:37<14:47,  8.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  47%|████▋     | 90/190 [13:49<16:04,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  48%|████▊     | 91/190 [14:00<16:54, 10.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  48%|████▊     | 92/190 [14:10<16:18,  9.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  49%|████▉     | 93/190 [14:17<14:42,  9.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  49%|████▉     | 94/190 [14:26<14:40,  9.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  50%|█████     | 95/190 [14:35<14:35,  9.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  51%|█████     | 96/190 [14:45<14:30,  9.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  51%|█████     | 97/190 [14:56<15:27,  9.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  52%|█████▏    | 98/190 [15:06<15:00,  9.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  52%|█████▏    | 99/190 [15:18<15:42, 10.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  53%|█████▎    | 100/190 [15:24<14:00,  9.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  53%|█████▎    | 101/190 [15:34<13:52,  9.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  54%|█████▎    | 102/190 [15:43<13:41,  9.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  54%|█████▍    | 103/190 [15:52<13:31,  9.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  55%|█████▍    | 104/190 [16:02<13:22,  9.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  55%|█████▌    | 105/190 [16:11<13:14,  9.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  56%|█████▌    | 106/190 [16:18<12:05,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  56%|█████▋    | 107/190 [16:25<11:16,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  57%|█████▋    | 108/190 [16:32<10:40,  7.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  57%|█████▋    | 109/190 [16:39<10:13,  7.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  58%|█████▊    | 110/190 [16:49<10:48,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  58%|█████▊    | 111/190 [16:58<11:08,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  59%|█████▉    | 112/190 [17:05<10:25,  8.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  59%|█████▉    | 113/190 [17:17<11:42,  9.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  60%|██████    | 114/190 [17:26<11:38,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  61%|██████    | 115/190 [17:35<11:32,  9.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  61%|██████    | 116/190 [17:47<12:17,  9.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  62%|██████▏   | 117/190 [17:59<12:44, 10.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  62%|██████▏   | 118/190 [18:10<12:59, 10.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  63%|██████▎   | 119/190 [18:20<12:17, 10.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  63%|██████▎   | 120/190 [18:31<12:34, 10.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  64%|██████▎   | 121/190 [18:41<11:53, 10.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  64%|██████▍   | 122/190 [18:48<10:35,  9.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  65%|██████▍   | 123/190 [18:59<11:12, 10.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  65%|██████▌   | 124/190 [19:06<10:02,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  66%|██████▌   | 125/190 [19:11<08:26,  7.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  66%|██████▋   | 126/190 [19:20<08:47,  8.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  67%|██████▋   | 127/190 [19:30<08:59,  8.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  67%|██████▋   | 128/190 [19:37<08:22,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  68%|██████▊   | 129/190 [19:46<08:38,  8.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  68%|██████▊   | 130/190 [19:58<09:28,  9.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  69%|██████▉   | 131/190 [20:07<09:16,  9.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  69%|██████▉   | 132/190 [20:16<09:05,  9.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  70%|███████   | 133/190 [20:28<09:35, 10.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  71%|███████   | 134/190 [20:35<08:33,  9.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  71%|███████   | 135/190 [20:44<08:26,  9.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  72%|███████▏  | 136/190 [20:54<08:19,  9.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  72%|███████▏  | 137/190 [20:58<06:57,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  73%|███████▎  | 138/190 [21:03<05:59,  6.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  73%|███████▎  | 139/190 [21:15<07:04,  8.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  74%|███████▎  | 140/190 [21:26<07:46,  9.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  74%|███████▍  | 141/190 [21:36<07:37,  9.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  75%|███████▍  | 142/190 [21:47<08:02, 10.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  75%|███████▌  | 143/190 [21:59<08:15, 10.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  76%|███████▌  | 144/190 [22:11<08:21, 10.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  76%|███████▋  | 145/190 [22:20<07:49, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  77%|███████▋  | 146/190 [22:27<06:53,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  77%|███████▋  | 147/190 [22:34<06:13,  8.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  78%|███████▊  | 148/190 [22:44<06:13,  8.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  78%|███████▊  | 149/190 [22:51<05:41,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  79%|███████▉  | 150/190 [23:02<06:12,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  79%|███████▉  | 151/190 [23:14<06:30, 10.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  80%|████████  | 152/190 [23:23<06:12,  9.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  81%|████████  | 153/190 [23:30<05:31,  8.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  81%|████████  | 154/190 [23:37<05:01,  8.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  82%|████████▏ | 155/190 [23:49<05:27,  9.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  82%|████████▏ | 156/190 [23:58<05:18,  9.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  83%|████████▎ | 157/190 [24:05<04:45,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  83%|████████▎ | 158/190 [24:15<04:43,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  84%|████████▎ | 159/190 [24:26<05:01,  9.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  84%|████████▍ | 160/190 [24:36<04:48,  9.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  85%|████████▍ | 161/190 [24:45<04:36,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  85%|████████▌ | 162/190 [24:52<04:05,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  86%|████████▌ | 163/190 [24:57<03:23,  7.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  86%|████████▋ | 164/190 [25:06<03:30,  8.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  87%|████████▋ | 165/190 [25:18<03:49,  9.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  87%|████████▋ | 166/190 [25:29<03:57,  9.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  88%|████████▊ | 167/190 [25:41<03:59, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  88%|████████▊ | 168/190 [25:50<03:42, 10.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  89%|████████▉ | 169/190 [26:02<03:41, 10.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  89%|████████▉ | 170/190 [26:09<03:09,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  90%|█████████ | 171/190 [26:18<02:59,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  91%|█████████ | 172/190 [26:28<02:49,  9.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  91%|█████████ | 173/190 [26:37<02:40,  9.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  92%|█████████▏| 174/190 [26:42<02:07,  8.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  92%|█████████▏| 175/190 [26:53<02:16,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  93%|█████████▎| 176/190 [27:03<02:08,  9.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  93%|█████████▎| 177/190 [27:10<01:50,  8.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  94%|█████████▎| 178/190 [27:19<01:45,  8.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  94%|█████████▍| 179/190 [27:26<01:30,  8.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  95%|█████████▍| 180/190 [27:35<01:25,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  95%|█████████▌| 181/190 [27:42<01:12,  8.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  96%|█████████▌| 182/190 [27:47<00:56,  7.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  96%|█████████▋| 183/190 [27:57<00:54,  7.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  97%|█████████▋| 184/190 [28:04<00:45,  7.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  97%|█████████▋| 185/190 [28:15<00:43,  8.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  98%|█████████▊| 186/190 [28:27<00:38,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  98%|█████████▊| 187/190 [28:36<00:28,  9.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  99%|█████████▉| 188/190 [28:48<00:20, 10.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3:  99%|█████████▉| 189/190 [28:55<00:09,  9.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3: 100%|██████████| 190/190 [29:02<00:00,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Mistral v1 SS - Batch 3: 100%|██████████| 190/190 [29:11<00:00,  9.22s/it]\n",
            "<ipython-input-68-d386eb5ea2e5>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  batch3[\"mlm_selected_sub\"] = batch3.progress_apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch3.to_csv(\"mistral_batch3.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"mistral_batch3.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wngZ0_zn-tsh",
        "outputId": "69fd3c84-91fe-4a25-b74b-2a5e09f33ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c6cb3e2c-5fe8-47d9-88d0-b688b16b9e35\", \"mistral_batch3.tsv\", 113803)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the three batches are combined together for testing"
      ],
      "metadata": {
        "id": "JlciBfDkV4Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "batch1 = pd.read_csv(\"/content/mistral_batch1.tsv\", sep=\"\\t\")\n",
        "batch2 = pd.read_csv(\"/content/mistral_batch2.tsv\", sep=\"\\t\")\n",
        "batch3 = pd.read_csv(\"/content/mistral_batch3.tsv\", sep=\"\\t\")\n",
        "\n",
        "# Merge all three\n",
        "merged_df = pd.concat([batch1, batch2, batch3], ignore_index=True)\n",
        "\n",
        "# Save merged file\n",
        "merged_df.to_csv(\"merged_mistral_substitutions.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"merged_mistral_substitutions.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-C-JJTrAGFrL",
        "outputId": "87b59c3f-544f-472c-a0a3-a6b3dba41f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e1189934-ffa9-40ca-8dbb-d5086df4e12c\", \"merged_mistral_substitutions.tsv\", 336614)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prompt_based_selection_at_k(df, k=10):\n",
        "    \"\"\"\n",
        "    Evaluate prompt-based selection assuming top-k selected substitutes (if available).\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with 'mlm_selected_sub' column (list of candidates).\n",
        "    - k: number of top substitutes to consider but the model selected only 1\n",
        "\n",
        "    Prints:\n",
        "    - Potential@k\n",
        "    - Precision@k\n",
        "    - Recall@k\n",
        "    \"\"\"\n",
        "    potential_hits = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_instances = 0\n",
        "\n",
        "    sub_cols = [col for col in df.columns if col.startswith(\"substitution_\")]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        gold_subs = set(str(s).lower() for s in row[sub_cols].dropna())\n",
        "        preds = row[\"mlm_selected_sub\"]\n",
        "\n",
        "        if isinstance(preds, str):\n",
        "            preds = [preds.strip().lower()]\n",
        "        elif isinstance(preds, list):\n",
        "            preds = [p.strip().lower() for p in preds[:k]]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if not gold_subs or not preds:\n",
        "            continue\n",
        "\n",
        "        gold_hit = len(set(preds) & gold_subs)\n",
        "        total_instances += 1\n",
        "        potential_hits += int(gold_hit > 0)\n",
        "        total_precision += gold_hit / k\n",
        "        total_recall += gold_hit / len(gold_subs)\n",
        "\n",
        "    print(f\"\\n Evaluation Results for Prompt-based SS at Top-{k}:\")\n",
        "    print(\" Potential@10:\", round(potential_hits / total_instances, 3))\n",
        "    print(\" Precision@10:\", round(total_precision / total_instances, 3))\n",
        "    print(\" Recall@10:\", round(total_recall / total_instances, 3))\n",
        "evaluate_prompt_based_selection_at_k(merged_df, k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iPc5xDr1WPh",
        "outputId": "681a9065-2125-4330-9084-ee747c2c82d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluation Results for Prompt-based SS at Top-10:\n",
            " Potential@10: 0.208\n",
            " Precision@10: 0.021\n",
            " Recall@10: 0.045\n"
          ]
        }
      ]
    }
  ]
}